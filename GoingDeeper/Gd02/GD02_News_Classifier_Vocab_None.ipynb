{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3edbd47",
   "metadata": {},
   "source": [
    "# 뉴스 카테고리 다중분류 프로젝트\n",
    "#### 주제\n",
    "- Vocabulary Size 변경해서 시도해보기\n",
    "    1. 모든 단어 사용\n",
    "    2. 빈도수 상위 5000개의 단어만 사용\n",
    "    3. 직접 단어 개수 설정해서 사용 \n",
    "- 모델 3가지 이상 사용: SVC, LinearRegression, RandomForest, Voting\n",
    "    - RNN, 1-D CNN 등 딥러닝 모델 중 하나 선택해서 머신러닝과 결과 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8877f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e588c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb038f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8e504897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19799290",
   "metadata": {},
   "source": [
    "# 1.모든 단어 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc193bf2",
   "metadata": {},
   "source": [
    "## 1-1. 데이터 로드 및 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a85046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d4b6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(x_train, y_train, x_test, y_test):\n",
    "    # 훈련 및 테스트 샘플 수 확인\n",
    "    print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
    "    print('테스트 샘플의 수: {}'.format(len(x_test)))\n",
    "    \n",
    "    # 클래스 수 확인\n",
    "    num_classes = max(y_train) + 1\n",
    "    print('클래스의 수 : {}'.format(num_classes))\n",
    "    \n",
    "    # 훈련 데이터 최대 길이 및 평균 길이 확인\n",
    "    print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "    print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    plt.hist([len(s) for s in x_train], bins=50)\n",
    "    plt.xlabel('length of samples')\n",
    "    plt.ylabel('number of samples')\n",
    "    plt.title(\"Length Distribution\")\n",
    "    \n",
    "    # 각 클래스 빈도수 확인\n",
    "    plt.subplot(212)\n",
    "    sns.countplot(x=y_train)\n",
    "    plt.title(\"Frequency of classes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1009aeed",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수: 8982\n",
      "테스트 샘플의 수: 2246\n",
      "클래스의 수 : 46\n",
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_data(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e92503fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(x_train, x_test):\n",
    "    # vocab 만들기\n",
    "    word_index = reuters.get_word_index(path = \"reuters_word_index.json\")\n",
    "    index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "    for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "        index_to_word[index] = token\n",
    "    \n",
    "    # index -> 원문 text 복원하기\n",
    "    x_train_decoded = []\n",
    "    for i in range(len(x_train)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "        x_train_decoded.append(t)\n",
    "    \n",
    "    x_test_decoded = []\n",
    "    for i in range(len(x_test)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "        x_test_decoded.append(t)\n",
    "    \n",
    "    dtmvector = CountVectorizer()\n",
    "    x_train_dtm = dtmvector.fit_transform(x_train_decoded)\n",
    "    x_test_dtm = dtmvector.transform(x_test_decoded)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x_train_tfidf = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "    x_test_tfidf = tfidf_transformer.transform(x_test_dtm)\n",
    "    \n",
    "    return x_train_tfidf, x_test_tfidf, x_train_decoded, x_test_decoded, word_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bbfa9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf, x_test_tfidf, x_train_decoded, x_test_decoded, word_index, index_to_word = vectorize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c9c2cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 26506)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd746263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 26506)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d01cc",
   "metadata": {},
   "source": [
    "## 1-2. 머신러닝 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85e2d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5ba7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47a861",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2581e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for logistic regression: 879.21 초\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lr = LogisticRegression(C=10000, penalty = \"l2\", max_iter = 3000)\n",
    "lr.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_lr = end_time - start_time \n",
    "\n",
    "print(f\"training time for logistic regression:{learning_time_lr: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d887995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.8165627782724845\n",
      "f1-score: 0.8114428402876209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.79      0.77       105\n",
      "           2       0.78      0.70      0.74        20\n",
      "           3       0.92      0.93      0.93       813\n",
      "           4       0.81      0.88      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.67      0.72      0.69        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.71      0.77      0.74        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.81      0.65      0.72        20\n",
      "          19       0.70      0.73      0.71       133\n",
      "          20       0.64      0.53      0.58        70\n",
      "          21       0.69      0.81      0.75        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.89      0.77      0.83        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       1.00      0.67      0.80        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.44      0.36      0.40        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.78      0.64      0.67      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lr.predict(x_test_tfidf)\n",
    "acc_lr = accuracy_score(y_test, predicted)\n",
    "f1_lr = f1_score(y_test, predicted, average = 'weighted')\n",
    "print(\"정확도: \", acc_lr)\n",
    "print(\"f1-score:\", f1_lr)\n",
    "print(classification_report(y_test, predicted, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4526ee8",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "787da934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for RandomForest: 2.14 초\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 5, random_state = 0)\n",
    "rf.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_rf = end_time - start_time \n",
    "\n",
    "print(f\"training time for RandomForest:{learning_time_rf: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6365efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6544968833481746\n",
      "f1-score: 0.6225909375608356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.58      0.35        12\n",
      "           1       0.35      0.60      0.44       105\n",
      "           2       0.32      0.40      0.36        20\n",
      "           3       0.82      0.89      0.85       813\n",
      "           4       0.62      0.84      0.71       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.51      0.47      0.49        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.46      0.20      0.28        30\n",
      "          11       0.56      0.64      0.60        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.33      0.16      0.22        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.46      0.52        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.44      0.20      0.28        20\n",
      "          19       0.61      0.50      0.55       133\n",
      "          20       0.51      0.33      0.40        70\n",
      "          21       0.55      0.22      0.32        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.33      0.08      0.13        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       1.00      0.23      0.37        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.43      0.27      0.33        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.65      2246\n",
      "   macro avg       0.40      0.25      0.28      2246\n",
      "weighted avg       0.63      0.65      0.62      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = rf.predict(x_test_tfidf)\n",
    "acc_rf = accuracy_score(y_test, predicted)\n",
    "f1_rf = f1_score(y_test, predicted, average = 'weighted')\n",
    "print(\"정확도:\", acc_rf) \n",
    "print(\"f1-score:\", f1_rf)\n",
    "print(classification_report(y_test, predicted, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9d67a",
   "metadata": {},
   "source": [
    "### linearSVC -> Soft-voting 적용 불가 (predict_proba 미제공)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96784260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for SVC: 104.05 초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "lsvc.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_svc = end_time - start_time\n",
    "\n",
    "print(f\"training time for SVC:{learning_time_svc: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3c25a4c7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LinearSVC instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66/2039636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0macc_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"정확도: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_svc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mclass\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LinearSVC instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "predicted = lsvc.predict(x_test_tfidf)\n",
    "acc_svc = accuracy_score(y_test, predicted)\n",
    "print(\"정확도: \", acc_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b5e29",
   "metadata": {},
   "source": [
    "### CalibratedClassifierCV 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c22c52e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for CalibratedSVC: 397.05 초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "start_time = time.time()\n",
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "calibrated_model = CalibratedClassifierCV(lsvc, cv = 5)\n",
    "calibrated_model.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_ccsvc = end_time - start_time\n",
    "\n",
    "print(f\"training time for CalibratedSVC:{learning_time_ccsvc: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "17ebe217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.7991985752448798\n",
      "f1-score:  0.7897576909777327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.75      0.75       105\n",
      "           2       0.68      0.75      0.71        20\n",
      "           3       0.89      0.94      0.91       813\n",
      "           4       0.78      0.88      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.65      0.68      0.67        38\n",
      "           9       0.91      0.80      0.85        25\n",
      "          10       0.96      0.83      0.89        30\n",
      "          11       0.66      0.73      0.70        83\n",
      "          12       0.50      0.31      0.38        13\n",
      "          13       0.64      0.57      0.60        37\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       0.67      0.22      0.33         9\n",
      "          16       0.70      0.73      0.71        99\n",
      "          17       0.80      0.33      0.47        12\n",
      "          18       0.93      0.65      0.76        20\n",
      "          19       0.64      0.72      0.68       133\n",
      "          20       0.60      0.41      0.49        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.70      0.58      0.64        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.92      0.71      0.80        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       0.88      0.54      0.67        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.75      0.27      0.40        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.40      0.57         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.80      0.58      0.64      2246\n",
      "weighted avg       0.80      0.80      0.79      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = calibrated_model.predict(x_test_tfidf)\n",
    "acc_ccsvc = accuracy_score(y_test, predicted)\n",
    "f1_ccsv = f1_score(y_test, predicted, average = 'weighted')\n",
    "print(\"정확도: \", acc_ccsvc)\n",
    "print(\"f1-score: \", f1_ccsv)\n",
    "print(classification_report(y_test, predicted, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185d790",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32e8c4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for SVC: 1232.88 초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators = [\n",
    "    ('lr', lr), ('rf', rf), ('ccsvc', calibrated_model)],\n",
    "    voting = 'soft'\n",
    ")\n",
    "voting_classifier.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_voting = end_time - start_time\n",
    "\n",
    "print(f\"training time for SVC:{learning_time_voting: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6fe3a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.8143365983971504\n",
      "f1-score:  0.8074185434484336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.74      0.80      0.77       105\n",
      "           2       0.74      0.70      0.72        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.79      0.88      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.70      0.74      0.72        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.65      0.75      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.66      0.62      0.64        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.70      0.77      0.73        99\n",
      "          17       0.88      0.58      0.70        12\n",
      "          18       0.93      0.65      0.76        20\n",
      "          19       0.70      0.71      0.71       133\n",
      "          20       0.63      0.51      0.57        70\n",
      "          21       0.72      0.78      0.75        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.67      0.67      0.67        12\n",
      "          24       0.71      0.53      0.61        19\n",
      "          25       0.92      0.71      0.80        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.60      0.30      0.40        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.67      0.80        12\n",
      "          31       0.89      0.62      0.73        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.50      0.27      0.35        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.79      0.60      0.65      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = voting_classifier.predict(x_test_tfidf)\n",
    "acc_voting = accuracy_score(y_test, predicted)\n",
    "f1_voting = f1_score(y_test, predicted, average = 'weighted')\n",
    "print(\"정확도: \", acc_voting)\n",
    "print(\"f1-score: \", f1_voting)\n",
    "print(classification_report(y_test, predicted, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d721e7",
   "metadata": {},
   "source": [
    "### DeepLearning - LSTM\n",
    "1. x_train, x_test 그대로 사용\n",
    "    - 로이터 데이터는 load할 때, num_words가 넘는 단어는 자동으로 <unk> 토큰 처리함\n",
    "2. sequence_length 비교 후 max_length 지정\n",
    "3. padding 토큰 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2548c17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  27595,\n",
       "  28842,\n",
       "  8,\n",
       "  43,\n",
       "  10,\n",
       "  447,\n",
       "  5,\n",
       "  25,\n",
       "  207,\n",
       "  270,\n",
       "  5,\n",
       "  3095,\n",
       "  111,\n",
       "  16,\n",
       "  369,\n",
       "  186,\n",
       "  90,\n",
       "  67,\n",
       "  7,\n",
       "  89,\n",
       "  5,\n",
       "  19,\n",
       "  102,\n",
       "  6,\n",
       "  19,\n",
       "  124,\n",
       "  15,\n",
       "  90,\n",
       "  67,\n",
       "  84,\n",
       "  22,\n",
       "  482,\n",
       "  26,\n",
       "  7,\n",
       "  48,\n",
       "  4,\n",
       "  49,\n",
       "  8,\n",
       "  864,\n",
       "  39,\n",
       "  209,\n",
       "  154,\n",
       "  6,\n",
       "  151,\n",
       "  6,\n",
       "  83,\n",
       "  11,\n",
       "  15,\n",
       "  22,\n",
       "  155,\n",
       "  11,\n",
       "  15,\n",
       "  7,\n",
       "  48,\n",
       "  9,\n",
       "  4579,\n",
       "  1005,\n",
       "  504,\n",
       "  6,\n",
       "  258,\n",
       "  6,\n",
       "  272,\n",
       "  11,\n",
       "  15,\n",
       "  22,\n",
       "  134,\n",
       "  44,\n",
       "  11,\n",
       "  15,\n",
       "  16,\n",
       "  8,\n",
       "  197,\n",
       "  1245,\n",
       "  90,\n",
       "  67,\n",
       "  52,\n",
       "  29,\n",
       "  209,\n",
       "  30,\n",
       "  32,\n",
       "  132,\n",
       "  6,\n",
       "  109,\n",
       "  15,\n",
       "  17,\n",
       "  12],\n",
       " [1,\n",
       "  4,\n",
       "  1378,\n",
       "  2025,\n",
       "  9,\n",
       "  697,\n",
       "  4622,\n",
       "  111,\n",
       "  8,\n",
       "  25,\n",
       "  109,\n",
       "  29,\n",
       "  3650,\n",
       "  11,\n",
       "  150,\n",
       "  244,\n",
       "  364,\n",
       "  33,\n",
       "  30,\n",
       "  30,\n",
       "  1398,\n",
       "  333,\n",
       "  6,\n",
       "  18292,\n",
       "  159,\n",
       "  9,\n",
       "  1084,\n",
       "  363,\n",
       "  13,\n",
       "  19231,\n",
       "  71,\n",
       "  9,\n",
       "  16273,\n",
       "  71,\n",
       "  117,\n",
       "  4,\n",
       "  225,\n",
       "  78,\n",
       "  206,\n",
       "  10,\n",
       "  9,\n",
       "  1214,\n",
       "  8,\n",
       "  4,\n",
       "  270,\n",
       "  5,\n",
       "  16273,\n",
       "  7,\n",
       "  748,\n",
       "  48,\n",
       "  9,\n",
       "  19231,\n",
       "  7,\n",
       "  207,\n",
       "  1451,\n",
       "  966,\n",
       "  1864,\n",
       "  793,\n",
       "  97,\n",
       "  133,\n",
       "  336,\n",
       "  7,\n",
       "  4,\n",
       "  493,\n",
       "  98,\n",
       "  273,\n",
       "  104,\n",
       "  284,\n",
       "  25,\n",
       "  39,\n",
       "  338,\n",
       "  22,\n",
       "  905,\n",
       "  220,\n",
       "  3465,\n",
       "  644,\n",
       "  59,\n",
       "  20,\n",
       "  6,\n",
       "  119,\n",
       "  61,\n",
       "  11,\n",
       "  15,\n",
       "  58,\n",
       "  579,\n",
       "  26,\n",
       "  10,\n",
       "  67,\n",
       "  7,\n",
       "  4,\n",
       "  738,\n",
       "  98,\n",
       "  43,\n",
       "  88,\n",
       "  333,\n",
       "  722,\n",
       "  12,\n",
       "  20,\n",
       "  6,\n",
       "  19,\n",
       "  746,\n",
       "  35,\n",
       "  15,\n",
       "  10,\n",
       "  9,\n",
       "  1214,\n",
       "  855,\n",
       "  129,\n",
       "  783,\n",
       "  21,\n",
       "  4,\n",
       "  2280,\n",
       "  244,\n",
       "  364,\n",
       "  51,\n",
       "  16,\n",
       "  299,\n",
       "  452,\n",
       "  16,\n",
       "  515,\n",
       "  4,\n",
       "  99,\n",
       "  29,\n",
       "  5,\n",
       "  4,\n",
       "  364,\n",
       "  281,\n",
       "  48,\n",
       "  10,\n",
       "  9,\n",
       "  1214,\n",
       "  23,\n",
       "  644,\n",
       "  47,\n",
       "  20,\n",
       "  324,\n",
       "  27,\n",
       "  56,\n",
       "  23406,\n",
       "  28185,\n",
       "  5,\n",
       "  192,\n",
       "  510,\n",
       "  17,\n",
       "  12])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b7a3729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data = list(x_train) + list(x_test)\n",
    "num_tokens = [len(index_list) for index_list in data]\n",
    "\n",
    "tokens_counts = Counter(num_tokens)\n",
    "lengths = sorted(tokens_counts.keys())\n",
    "frequencies = [tokens_counts[length] for length in lengths]\n",
    "\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.bar(lengths, frequencies, color = 'blue')\n",
    "plt.title(\"Sentence Length Frequency\")\n",
    "plt.xlabel(\"Sentence Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86a1f277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length : 2\n",
      "max length : 2376\n",
      "avg length : 145.96419665122906\n",
      "median length : 95.0\n"
     ]
    }
   ],
   "source": [
    "print(\"min length :\", np.min(num_tokens))\n",
    "print(\"max length :\", np.max(num_tokens))\n",
    "print(\"avg length :\", np.average(num_tokens))\n",
    "print(\"median length :\", np.median(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff04d137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 13, 15, 16, 17, 18, 19, 20, 21, 22]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하위 length 길이 확인\n",
    "lengths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "283ab829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437.7211495431185"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평균 + 2*표준편차\n",
    "np.average(num_tokens) + 2*np.std(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f47efdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minlen: 10\n",
      "maxlen: 440\n",
      "valid %: 94.43% \n"
     ]
    }
   ],
   "source": [
    "minlen = 10\n",
    "maxlen = 440\n",
    "\n",
    "valid_cnt = len([length for length in num_tokens if minlen <= length <= maxlen])\n",
    "print(f'''minlen: {minlen}\n",
    "maxlen: {maxlen}\n",
    "valid %:{valid_cnt/len(num_tokens)*100: .2f}% ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb8c7c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 x_train 개수:  8982\n",
      "토큰개수 2 이하 시퀀스 삭제 후 x_train 개수:  8982\n"
     ]
    }
   ],
   "source": [
    "# x_train에서 토큰 개수 2이하인 데이터 삭제, max_length = 440 지정하여 패딩처리\n",
    "print(\"원본 x_train 개수: \", len(x_train))\n",
    "x_train = [index_list for index_list in x_train if len(index_list) > 3]\n",
    "print(\"토큰개수 2 이하 시퀀스 삭제 후 x_train 개수: \", len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7e9e6",
   "metadata": {},
   "source": [
    "- x_train에는 2이하의 토큰이 없었던 것 같다!\n",
    "- 2 이하의 시퀀스는 test 데이터셋에만 포함된 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd100f",
   "metadata": {},
   "source": [
    "**최소 길이 데이터셋 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8379e0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tmp = [index_list for index_list in x_test if len(index_list) <= 2]\n",
    "len(x_test_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5e1d8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f00cf6",
   "metadata": {},
   "source": [
    "**패딩 추가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d334e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_word 사용해서 word_to_index 만들기\n",
    "word_to_index = {word:index for index, word in index_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "311cd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, value = word_to_index[\"<pad>\"], padding = 'pre', maxlen = maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, value = word_to_index[\"<pad>\"], padding = 'pre', maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d3337",
   "metadata": {},
   "source": [
    "**train, val 데이터셋 나누기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba5cded5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7185, 440), (7185,), (1797, 440), (1797,))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_partial, x_val, y_train_partial, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 2024)\n",
    "x_train_partial.shape, y_train_partial.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5299cf",
   "metadata": {},
   "source": [
    "### 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a99ce839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 레이블 개수\n",
    "np.max(y_train)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4afaa036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         3965696   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 4,108,526\n",
      "Trainable params: 4,108,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "word_vector_dim = 128\n",
    "\n",
    "model_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape = (None, )),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(64, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(46, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f7d5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"best_model_rnn.keras\", monitor = \"val_accuracy\", verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "24bed788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 39s 203ms/step - loss: 3.5819 - accuracy: 0.3240 - val_loss: 2.9010 - val_accuracy: 0.3589\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.35893, saving model to best_model_rnn.keras\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 2.7207 - accuracy: 0.3094 - val_loss: 2.5709 - val_accuracy: 0.3589\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.35893\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 2.4025 - accuracy: 0.3587 - val_loss: 2.2421 - val_accuracy: 0.4079\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.35893 to 0.40790, saving model to best_model_rnn.keras\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 2.0827 - accuracy: 0.4029 - val_loss: 2.0258 - val_accuracy: 0.4213\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.40790 to 0.42126, saving model to best_model_rnn.keras\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.8897 - accuracy: 0.4841 - val_loss: 1.9113 - val_accuracy: 0.4858\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.42126 to 0.48581, saving model to best_model_rnn.keras\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 1.8098 - accuracy: 0.5084 - val_loss: 1.8660 - val_accuracy: 0.4969\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.48581 to 0.49694, saving model to best_model_rnn.keras\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 1.6622 - accuracy: 0.5353 - val_loss: 1.8638 - val_accuracy: 0.5192\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.49694 to 0.51920, saving model to best_model_rnn.keras\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.5295 - accuracy: 0.5784 - val_loss: 1.8171 - val_accuracy: 0.5264\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.51920 to 0.52643, saving model to best_model_rnn.keras\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 1.4715 - accuracy: 0.5886 - val_loss: 1.8353 - val_accuracy: 0.5120\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.52643\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 1.3659 - accuracy: 0.6213 - val_loss: 2.4443 - val_accuracy: 0.4335\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.52643\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 1.5457 - accuracy: 0.5765 - val_loss: 1.8109 - val_accuracy: 0.5415\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.52643 to 0.54146, saving model to best_model_rnn.keras\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 1.3187 - accuracy: 0.6377 - val_loss: 1.8167 - val_accuracy: 0.5504\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.54146 to 0.55036, saving model to best_model_rnn.keras\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 2.1986 - accuracy: 0.4217 - val_loss: 2.5456 - val_accuracy: 0.2176\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.55036\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 2.1739 - accuracy: 0.3592 - val_loss: 2.2105 - val_accuracy: 0.3912\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.55036\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 1.9737 - accuracy: 0.3880 - val_loss: 2.1307 - val_accuracy: 0.3984\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.55036\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 2.0636 - accuracy: 0.3752 - val_loss: 2.0601 - val_accuracy: 0.4619\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.55036\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.8134 - accuracy: 0.5086 - val_loss: 1.8730 - val_accuracy: 0.4936\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.55036\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 1.5358 - accuracy: 0.5669 - val_loss: 1.9233 - val_accuracy: 0.5025\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.55036\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 1.4568 - accuracy: 0.5935 - val_loss: 1.9195 - val_accuracy: 0.5070\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.55036\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.4105 - accuracy: 0.5976 - val_loss: 1.8984 - val_accuracy: 0.4986\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.55036\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.3494 - accuracy: 0.6170 - val_loss: 1.8523 - val_accuracy: 0.5253\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.55036\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 1.2345 - accuracy: 0.6498 - val_loss: 2.3325 - val_accuracy: 0.5314\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.55036\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 1.5627 - accuracy: 0.5833 - val_loss: 1.7895 - val_accuracy: 0.5426\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.55036\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 1.2686 - accuracy: 0.6398 - val_loss: 1.8476 - val_accuracy: 0.5470\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.55036\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 1.1710 - accuracy: 0.6696 - val_loss: 1.8914 - val_accuracy: 0.5437\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.55036\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 1.1218 - accuracy: 0.6966 - val_loss: 1.9346 - val_accuracy: 0.5437\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.55036\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 1.0722 - accuracy: 0.7035 - val_loss: 1.9093 - val_accuracy: 0.5587\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.55036 to 0.55871, saving model to best_model_rnn.keras\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 1.0294 - accuracy: 0.7276 - val_loss: 1.9745 - val_accuracy: 0.5576\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.55871\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.9896 - accuracy: 0.7365 - val_loss: 2.0116 - val_accuracy: 0.5487\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.55871\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.9474 - accuracy: 0.7457 - val_loss: 2.0395 - val_accuracy: 0.5637\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.55871 to 0.56372, saving model to best_model_rnn.keras\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.9047 - accuracy: 0.7545 - val_loss: 2.1110 - val_accuracy: 0.5526\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.56372\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.8661 - accuracy: 0.7651 - val_loss: 2.1267 - val_accuracy: 0.5643\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.56372 to 0.56427, saving model to best_model_rnn.keras\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.8278 - accuracy: 0.7720 - val_loss: 2.2268 - val_accuracy: 0.5609\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.56427\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.7867 - accuracy: 0.7807 - val_loss: 2.2866 - val_accuracy: 0.5665\n",
      "\n",
      "Epoch 00034: val_accuracy improved from 0.56427 to 0.56650, saving model to best_model_rnn.keras\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.7582 - accuracy: 0.7871 - val_loss: 2.3720 - val_accuracy: 0.5615\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.56650\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.7501 - accuracy: 0.7901 - val_loss: 2.4063 - val_accuracy: 0.5643\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.56650\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.7446 - accuracy: 0.7911 - val_loss: 2.5273 - val_accuracy: 0.5587\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.56650\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.7527 - accuracy: 0.7953 - val_loss: 2.4877 - val_accuracy: 0.5576\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.56650\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.7481 - accuracy: 0.7916 - val_loss: 2.5812 - val_accuracy: 0.5576\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.56650\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.6931 - accuracy: 0.8104 - val_loss: 2.5113 - val_accuracy: 0.5593\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.56650\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.6518 - accuracy: 0.8309 - val_loss: 2.5844 - val_accuracy: 0.5698\n",
      "\n",
      "Epoch 00041: val_accuracy improved from 0.56650 to 0.56984, saving model to best_model_rnn.keras\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.6208 - accuracy: 0.8402 - val_loss: 2.6361 - val_accuracy: 0.5598\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.56984\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.5946 - accuracy: 0.8482 - val_loss: 2.7124 - val_accuracy: 0.5637\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.56984\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.5695 - accuracy: 0.8514 - val_loss: 2.7939 - val_accuracy: 0.5698\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.56984\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.5922 - accuracy: 0.8390 - val_loss: 2.8127 - val_accuracy: 0.5637\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.56984\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.5435 - accuracy: 0.8562 - val_loss: 2.9049 - val_accuracy: 0.5587\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.56984\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.5105 - accuracy: 0.8728 - val_loss: 2.9524 - val_accuracy: 0.5626\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.56984\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.4824 - accuracy: 0.8823 - val_loss: 3.0263 - val_accuracy: 0.5710\n",
      "\n",
      "Epoch 00048: val_accuracy improved from 0.56984 to 0.57095, saving model to best_model_rnn.keras\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.4822 - accuracy: 0.8834 - val_loss: 3.1032 - val_accuracy: 0.5654\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.57095\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.4514 - accuracy: 0.8917 - val_loss: 3.1806 - val_accuracy: 0.5654\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.57095\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.4308 - accuracy: 0.8912 - val_loss: 3.1374 - val_accuracy: 0.5687\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.57095\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.4154 - accuracy: 0.8963 - val_loss: 3.2238 - val_accuracy: 0.5676\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.57095\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.3977 - accuracy: 0.9010 - val_loss: 3.2841 - val_accuracy: 0.5682\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.57095\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.3860 - accuracy: 0.9054 - val_loss: 3.2714 - val_accuracy: 0.5721\n",
      "\n",
      "Epoch 00054: val_accuracy improved from 0.57095 to 0.57206, saving model to best_model_rnn.keras\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.3808 - accuracy: 0.9062 - val_loss: 3.5220 - val_accuracy: 0.5504\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.57206\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.3852 - accuracy: 0.9051 - val_loss: 3.3264 - val_accuracy: 0.5682\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.57206\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.4090 - accuracy: 0.8932 - val_loss: 3.4992 - val_accuracy: 0.5604\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.57206\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.3943 - accuracy: 0.8937 - val_loss: 3.5433 - val_accuracy: 0.5593\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.57206\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.4043 - accuracy: 0.8913 - val_loss: 3.3622 - val_accuracy: 0.5637\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.57206\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.3641 - accuracy: 0.9080 - val_loss: 3.5542 - val_accuracy: 0.5531\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.57206\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 2s 116ms/step - loss: 0.3612 - accuracy: 0.9058 - val_loss: 3.3441 - val_accuracy: 0.5715\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.57206\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.3282 - accuracy: 0.9164 - val_loss: 3.2963 - val_accuracy: 0.5804\n",
      "\n",
      "Epoch 00062: val_accuracy improved from 0.57206 to 0.58041, saving model to best_model_rnn.keras\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.3389 - accuracy: 0.9122 - val_loss: 3.4838 - val_accuracy: 0.5609\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.58041\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.3267 - accuracy: 0.9201 - val_loss: 3.5325 - val_accuracy: 0.5637\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.58041\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.3023 - accuracy: 0.9243 - val_loss: 3.4192 - val_accuracy: 0.5704\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.58041\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 0.2804 - accuracy: 0.9332 - val_loss: 3.4568 - val_accuracy: 0.5704\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.58041\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.2654 - accuracy: 0.9367 - val_loss: 3.5436 - val_accuracy: 0.5687\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.58041\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.2509 - accuracy: 0.9403 - val_loss: 3.6221 - val_accuracy: 0.5693\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.58041\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 2s 113ms/step - loss: 0.2617 - accuracy: 0.9364 - val_loss: 3.7646 - val_accuracy: 0.5609\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.58041\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 2s 116ms/step - loss: 0.2545 - accuracy: 0.9367 - val_loss: 3.8122 - val_accuracy: 0.5565\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.58041\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.2358 - accuracy: 0.9453 - val_loss: 3.6881 - val_accuracy: 0.5676\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.58041\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.2438 - accuracy: 0.9404 - val_loss: 3.8332 - val_accuracy: 0.5665\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.58041\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.2267 - accuracy: 0.9478 - val_loss: 3.8989 - val_accuracy: 0.5565\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.58041\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.2098 - accuracy: 0.9498 - val_loss: 3.8178 - val_accuracy: 0.5715\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.58041\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 2s 119ms/step - loss: 0.2119 - accuracy: 0.9474 - val_loss: 3.8165 - val_accuracy: 0.5704\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.58041\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2052 - accuracy: 0.9510 - val_loss: 3.9848 - val_accuracy: 0.5593\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.58041\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.2062 - accuracy: 0.9492 - val_loss: 4.0739 - val_accuracy: 0.5726\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.58041\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.2250 - accuracy: 0.9432 - val_loss: 3.8896 - val_accuracy: 0.5582\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.58041\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.2193 - accuracy: 0.9484 - val_loss: 3.9812 - val_accuracy: 0.5710\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.58041\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.1907 - accuracy: 0.9546 - val_loss: 4.0094 - val_accuracy: 0.5626\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.58041\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.1814 - accuracy: 0.9557 - val_loss: 4.1017 - val_accuracy: 0.5531\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.58041\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.1817 - accuracy: 0.9545 - val_loss: 4.0878 - val_accuracy: 0.5654\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.58041\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.1681 - accuracy: 0.9580 - val_loss: 4.0594 - val_accuracy: 0.5693\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.58041\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.1573 - accuracy: 0.9598 - val_loss: 4.1379 - val_accuracy: 0.5626\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.58041\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.1538 - accuracy: 0.9606 - val_loss: 4.1553 - val_accuracy: 0.5721\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.58041\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.1518 - accuracy: 0.9605 - val_loss: 4.2305 - val_accuracy: 0.5576\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.58041\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 0.1473 - accuracy: 0.9633 - val_loss: 4.1369 - val_accuracy: 0.5710\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.58041\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1472 - accuracy: 0.9628 - val_loss: 4.2765 - val_accuracy: 0.5543\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.58041\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.1917 - accuracy: 0.9517 - val_loss: 4.1321 - val_accuracy: 0.5565\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.58041\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.1858 - accuracy: 0.9545 - val_loss: 4.2920 - val_accuracy: 0.5570\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.58041\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.1670 - accuracy: 0.9564 - val_loss: 4.2461 - val_accuracy: 0.5582\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.58041\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.1474 - accuracy: 0.9610 - val_loss: 4.0958 - val_accuracy: 0.5698\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.58041\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.1466 - accuracy: 0.9605 - val_loss: 4.2553 - val_accuracy: 0.5620\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.58041\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.1429 - accuracy: 0.9602 - val_loss: 4.3414 - val_accuracy: 0.5598\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.58041\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1457 - accuracy: 0.9610 - val_loss: 4.2819 - val_accuracy: 0.5659\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.58041\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.1619 - accuracy: 0.9576 - val_loss: 4.1460 - val_accuracy: 0.5721\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.58041\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.1448 - accuracy: 0.9623 - val_loss: 4.2646 - val_accuracy: 0.5609\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.58041\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.1297 - accuracy: 0.9635 - val_loss: 4.2386 - val_accuracy: 0.5659\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.58041\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 0.1224 - accuracy: 0.9658 - val_loss: 4.2896 - val_accuracy: 0.5693\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.58041\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.1222 - accuracy: 0.9652 - val_loss: 4.1720 - val_accuracy: 0.5765\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.58041\n"
     ]
    }
   ],
   "source": [
    "model_rnn.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "history_rnn = model_rnn.fit(x_train_partial, \n",
    "                            y_train_partial, \n",
    "                            epochs = 100, \n",
    "                            batch_size = 512,\n",
    "                            validation_data = (x_val, y_val),\n",
    "                            callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5aa973d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 - 1s - loss: 3.3976 - accuracy: 0.5788\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_rnn = load_model(\"best_model_rnn.keras\")\n",
    "results_rnn = model_rnn.evaluate(x_test, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0353d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.17      0.21        12\n",
      "           1       0.41      0.30      0.35       105\n",
      "           2       0.18      0.15      0.16        20\n",
      "           3       0.92      0.88      0.90       813\n",
      "           4       0.68      0.82      0.74       474\n",
      "           5       0.07      0.20      0.11         5\n",
      "           6       0.75      0.21      0.33        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.14      0.24      0.18        38\n",
      "           9       0.33      0.12      0.18        25\n",
      "          10       0.12      0.07      0.09        30\n",
      "          11       0.20      0.24      0.22        83\n",
      "          12       0.11      0.15      0.13        13\n",
      "          13       0.07      0.08      0.07        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.25      0.21      0.23        99\n",
      "          17       0.04      0.08      0.06        12\n",
      "          18       0.07      0.05      0.06        20\n",
      "          19       0.56      0.41      0.47       133\n",
      "          20       0.17      0.23      0.19        70\n",
      "          21       0.12      0.15      0.13        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.09      0.16      0.12        19\n",
      "          25       0.27      0.32      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.17      0.14      0.15         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.08      0.09      0.08        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.14      0.10      0.12        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       1.00      0.20      0.33         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.58      2246\n",
      "   macro avg       0.18      0.15      0.15      2246\n",
      "weighted avg       0.58      0.58      0.57      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model_rnn.predict(x_test)\n",
    "predicted_classes = np.argmax(predicted, axis = 1)\n",
    "print(classification_report(y_test, predicted_classes, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
