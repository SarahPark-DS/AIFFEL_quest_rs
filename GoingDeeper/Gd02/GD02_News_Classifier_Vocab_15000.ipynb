{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175d30ed",
   "metadata": {},
   "source": [
    "# 뉴스 카테고리 다중분류 프로젝트 (Vocab = 15000)\n",
    "#### 주제\n",
    "- Vocabulary Size 변경해서 시도해보기\n",
    "    1. 모든 단어 사용\n",
    "    2. 빈도수 상위 5000개의 단어만 사용\n",
    "    3. 직접 단어 개수 설정해서 사용 \n",
    "- 모델 3가지 이상 사용: SVC, LinearRegression, RandomForest, Voting\n",
    "    - RNN, 1-D CNN 등 딥러닝 모델 중 하나 선택해서 머신러닝과 결과 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b75c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45cd0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a6d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1547b6",
   "metadata": {},
   "source": [
    "# 1.모든 단어 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a13ea",
   "metadata": {},
   "source": [
    "## 1-1. 데이터 로드 및 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a98983",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=15000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61cf016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(x_train, y_train, x_test, y_test):\n",
    "    # 훈련 및 테스트 샘플 수 확인\n",
    "    print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
    "    print('테스트 샘플의 수: {}'.format(len(x_test)))\n",
    "    \n",
    "    # 클래스 수 확인\n",
    "    num_classes = max(y_train) + 1\n",
    "    print('클래스의 수 : {}'.format(num_classes))\n",
    "    \n",
    "    # 훈련 데이터 최대 길이 및 평균 길이 확인\n",
    "    print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "    print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    plt.hist([len(s) for s in x_train], bins=50)\n",
    "    plt.xlabel('length of samples')\n",
    "    plt.ylabel('number of samples')\n",
    "    plt.title(\"Length Distribution\")\n",
    "    \n",
    "    # 각 클래스 빈도수 확인\n",
    "    plt.subplot(212)\n",
    "    sns.countplot(x=y_train)\n",
    "    plt.title(\"Frequency of classes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7eced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수: 8982\n",
      "테스트 샘플의 수: 2246\n",
      "클래스의 수 : 46\n",
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_data(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59cf9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(x_train, x_test):\n",
    "    # vocab 만들기\n",
    "    word_index = reuters.get_word_index(path = \"reuters_word_index.json\")\n",
    "    index_to_word = {index + 3 : word for word, index in word_index.items()}\n",
    "    for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "        index_to_word[index] = token\n",
    "    \n",
    "    # index -> 원문 text 복원하기\n",
    "    x_train_decoded = []\n",
    "    for i in range(len(x_train)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "        x_train_decoded.append(t)\n",
    "    \n",
    "    x_test_decoded = []\n",
    "    for i in range(len(x_test)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "        x_test_decoded.append(t)\n",
    "    \n",
    "    dtmvector = CountVectorizer()\n",
    "    x_train_dtm = dtmvector.fit_transform(x_train_decoded)\n",
    "    x_test_dtm = dtmvector.transform(x_test_decoded)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x_train_tfidf = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "    x_test_tfidf = tfidf_transformer.transform(x_test_dtm)\n",
    "    \n",
    "    return x_train_tfidf, x_test_tfidf, x_train_decoded, x_test_decoded, word_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85616cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf, x_test_tfidf, x_train_decoded, x_test_decoded, word_index, index_to_word = vectorize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd2ff59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982, 14227)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca44e12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 14227)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779757d",
   "metadata": {},
   "source": [
    "## 1-2. 머신러닝 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de9ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8520e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd6519",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1b2b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for logistic regression: 885.67 초\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lr = LogisticRegression(C=10000, penalty = \"l2\", max_iter = 3000)\n",
    "lr.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_lr = end_time - start_time \n",
    "\n",
    "print(f\"training time for logistic regression:{learning_time_lr: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e3cf986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.8147818343722173\n",
      "f1-score: 0.809871785370617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.73      0.79      0.76       105\n",
      "           2       0.78      0.70      0.74        20\n",
      "           3       0.92      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.71      0.71        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.68      0.72      0.70        83\n",
      "          12       0.62      0.38      0.48        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.70      0.77      0.73        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.70      0.73      0.72       133\n",
      "          20       0.63      0.53      0.57        70\n",
      "          21       0.69      0.81      0.75        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.88      0.74      0.81        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.37        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.58      0.74        12\n",
      "          31       0.78      0.54      0.64        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.50      0.45      0.48        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.78      0.63      0.67      2246\n",
      "weighted avg       0.82      0.81      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lr.predict(x_test_tfidf)\n",
    "acc_lr = accuracy_score(y_test, predicted)\n",
    "f1_lr = f1_score(y_test, predicted, average = 'weighted')\n",
    "print(\"정확도: \", acc_lr)\n",
    "print(\"f1-score:\", f1_lr)\n",
    "print(classification_report(y_test, predicted, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a2a35",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cccbaaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for RandomForest: 1.77 초\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 5, random_state = 0)\n",
    "rf.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_rf = end_time - start_time \n",
    "\n",
    "print(f\"training time for RandomForest:{learning_time_rf: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3af75fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6714158504007124\n",
      "f1-score: 0.6406930098492383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.50      0.33        12\n",
      "           1       0.42      0.69      0.52       105\n",
      "           2       0.13      0.10      0.11        20\n",
      "           3       0.81      0.90      0.85       813\n",
      "           4       0.65      0.85      0.74       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.83      0.36      0.50        14\n",
      "           7       0.50      0.67      0.57         3\n",
      "           8       0.62      0.53      0.57        38\n",
      "           9       0.71      0.40      0.51        25\n",
      "          10       0.60      0.20      0.30        30\n",
      "          11       0.45      0.51      0.47        83\n",
      "          12       0.60      0.23      0.33        13\n",
      "          13       0.40      0.22      0.28        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.55      0.46      0.50        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.30      0.39        20\n",
      "          19       0.65      0.56      0.60       133\n",
      "          20       0.60      0.37      0.46        70\n",
      "          21       0.56      0.33      0.42        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.50      0.17      0.25        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       1.00      0.35      0.52        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       1.00      0.10      0.18        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.20      0.33        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.40      0.18      0.25        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.50      0.12      0.20         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.33      0.17      0.22         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.43      0.26      0.30      2246\n",
      "weighted avg       0.65      0.67      0.64      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = rf.predict(x_test_tfidf)\n",
    "acc_rf = accuracy_score(y_test, predicted)\n",
    "f1_rf = f1_score(y_test, predicted, average = 'weighted')\n",
    "print(\"정확도:\", acc_rf) \n",
    "print(\"f1-score:\", f1_rf)\n",
    "print(classification_report(y_test, predicted, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9c9e5",
   "metadata": {},
   "source": [
    "### CalibratedClassifierCV + linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f87cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for CalibratedSVC: 413.00 초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "start_time = time.time()\n",
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "calibrated_model = CalibratedClassifierCV(lsvc, cv = 5)\n",
    "calibrated_model.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_ccsvc = end_time - start_time\n",
    "\n",
    "print(f\"training time for CalibratedSVC:{learning_time_ccsvc: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33305b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.7983081032947462\n",
      "f1-score:  0.7884541763058542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.75      0.75      0.75       105\n",
      "           2       0.74      0.70      0.72        20\n",
      "           3       0.88      0.93      0.90       813\n",
      "           4       0.79      0.88      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.62      0.66      0.64        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.96      0.87      0.91        30\n",
      "          11       0.65      0.75      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.65      0.54      0.59        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       1.00      0.22      0.36         9\n",
      "          16       0.70      0.75      0.72        99\n",
      "          17       1.00      0.42      0.59        12\n",
      "          18       0.93      0.65      0.76        20\n",
      "          19       0.64      0.73      0.68       133\n",
      "          20       0.62      0.43      0.51        70\n",
      "          21       0.61      0.74      0.67        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.70      0.58      0.64        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.60      0.30      0.40        10\n",
      "          29       0.40      0.50      0.44         4\n",
      "          30       1.00      0.25      0.40        12\n",
      "          31       0.88      0.54      0.67        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.75      0.27      0.40        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       1.00      0.40      0.57         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.80      0.57      0.63      2246\n",
      "weighted avg       0.80      0.80      0.79      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = calibrated_model.predict(x_test_tfidf)\n",
    "acc_ccsvc = accuracy_score(y_test, predicted)\n",
    "f1_ccsv = f1_score(y_test, predicted, average = 'weighted')\n",
    "print(\"정확도: \", acc_ccsvc)\n",
    "print(\"f1-score: \", f1_ccsv)\n",
    "print(classification_report(y_test, predicted, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1906a3",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8e7a644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for SVC: 1178.42 초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators = [\n",
    "    ('lr', lr), ('rf', rf), ('ccsvc', calibrated_model)],\n",
    "    voting = 'soft'\n",
    ")\n",
    "voting_classifier.fit(x_train_tfidf, y_train)\n",
    "end_time = time.time()\n",
    "learning_time_voting = end_time - start_time\n",
    "\n",
    "print(f\"training time for SVC:{learning_time_voting: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3db6091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.8147818343722173\n",
      "f1-score:  0.80731161357066\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.74      0.79      0.76       105\n",
      "           2       0.82      0.70      0.76        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.80      0.89      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.70      0.74      0.72        38\n",
      "           9       0.85      0.88      0.86        25\n",
      "          10       0.93      0.83      0.88        30\n",
      "          11       0.67      0.73      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.65      0.65      0.65        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.80      0.44      0.57         9\n",
      "          16       0.72      0.77      0.75        99\n",
      "          17       0.88      0.58      0.70        12\n",
      "          18       0.87      0.65      0.74        20\n",
      "          19       0.69      0.73      0.71       133\n",
      "          20       0.61      0.50      0.55        70\n",
      "          21       0.69      0.81      0.75        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.75      0.75      0.75        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.92      0.74      0.82        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.60      0.30      0.40        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       0.88      0.54      0.67        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.57      0.36      0.44        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.20      0.29         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.79      0.60      0.65      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = voting_classifier.predict(x_test_tfidf)\n",
    "acc_voting = accuracy_score(y_test, predicted)\n",
    "f1_voting = f1_score(y_test, predicted, average = 'weighted')\n",
    "print(\"정확도: \", acc_voting)\n",
    "print(\"f1-score: \", f1_voting)\n",
    "print(classification_report(y_test, predicted, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c891528",
   "metadata": {},
   "source": [
    "### DeepLearning - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe4690e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 추가\n",
    "maxlen = 440\n",
    "\n",
    "word_to_index = {word:index for index, word in index_to_word.items()}\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, value = word_to_index[\"<pad>\"], padding = 'pre', maxlen = maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, value = word_to_index[\"<pad>\"], padding = 'pre', maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd27033e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7185, 440), (7185,), (1797, 440), (1797,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test, val 나누기 \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_partial, x_val, y_train_partial, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 2024)\n",
    "x_train_partial.shape, y_train_partial.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdcb1f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4e0afd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1920000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 2,062,830\n",
      "Trainable params: 2,062,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "vocab_size = 15000\n",
    "word_vector_dim = 128\n",
    "\n",
    "model_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape = (None, )),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(64, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(46, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63c180cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"best_model_rnn.keras\", monitor = \"val_accuracy\", verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "640435a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 5s 127ms/step - loss: 3.4188 - accuracy: 0.3283 - val_loss: 2.7783 - val_accuracy: 0.3589\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.35893, saving model to best_model_rnn.keras\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 2.5923 - accuracy: 0.3499 - val_loss: 2.4819 - val_accuracy: 0.3589\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.35893\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 2.4437 - accuracy: 0.3499 - val_loss: 2.4171 - val_accuracy: 0.3589\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.35893\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 2.4152 - accuracy: 0.3499 - val_loss: 2.4043 - val_accuracy: 0.3589\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.35893\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 2.4012 - accuracy: 0.3499 - val_loss: 2.3851 - val_accuracy: 0.3589\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.35893\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 2.3666 - accuracy: 0.3500 - val_loss: 2.2465 - val_accuracy: 0.3589\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.35893\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 2.1158 - accuracy: 0.3912 - val_loss: 1.9903 - val_accuracy: 0.4652\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.35893 to 0.46522, saving model to best_model_rnn.keras\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 1.9005 - accuracy: 0.4839 - val_loss: 2.0040 - val_accuracy: 0.4146\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.46522\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 1.8277 - accuracy: 0.5164 - val_loss: 1.8144 - val_accuracy: 0.5086\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.46522 to 0.50863, saving model to best_model_rnn.keras\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.7098 - accuracy: 0.5535 - val_loss: 1.7969 - val_accuracy: 0.5348\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.50863 to 0.53478, saving model to best_model_rnn.keras\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.6582 - accuracy: 0.5715 - val_loss: 1.7359 - val_accuracy: 0.5515\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.53478 to 0.55147, saving model to best_model_rnn.keras\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 1.6244 - accuracy: 0.5755 - val_loss: 1.7899 - val_accuracy: 0.5303\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.55147\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.6467 - accuracy: 0.5645 - val_loss: 1.7706 - val_accuracy: 0.5209\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.55147\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.6156 - accuracy: 0.5637 - val_loss: 1.7603 - val_accuracy: 0.5337\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.55147\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.6222 - accuracy: 0.5747 - val_loss: 1.8235 - val_accuracy: 0.5359\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.55147\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.5132 - accuracy: 0.6120 - val_loss: 1.7373 - val_accuracy: 0.5570\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.55147 to 0.55704, saving model to best_model_rnn.keras\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.4247 - accuracy: 0.6284 - val_loss: 1.6969 - val_accuracy: 0.5682\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.55704 to 0.56817, saving model to best_model_rnn.keras\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.4107 - accuracy: 0.6231 - val_loss: 1.6895 - val_accuracy: 0.5687\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.56817 to 0.56873, saving model to best_model_rnn.keras\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.3056 - accuracy: 0.6484 - val_loss: 1.6653 - val_accuracy: 0.5810\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.56873 to 0.58097, saving model to best_model_rnn.keras\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.2365 - accuracy: 0.6717 - val_loss: 1.6708 - val_accuracy: 0.5826\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.58097 to 0.58264, saving model to best_model_rnn.keras\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.1910 - accuracy: 0.6791 - val_loss: 1.7971 - val_accuracy: 0.5910\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.58264 to 0.59099, saving model to best_model_rnn.keras\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 1.2270 - accuracy: 0.6615 - val_loss: 1.6787 - val_accuracy: 0.5776\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.59099\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.1191 - accuracy: 0.6976 - val_loss: 1.8184 - val_accuracy: 0.5659\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.59099\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 1.1603 - accuracy: 0.6820 - val_loss: 1.7671 - val_accuracy: 0.5559\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.59099\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 1.0596 - accuracy: 0.7049 - val_loss: 1.7449 - val_accuracy: 0.5843\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.59099\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.9898 - accuracy: 0.7292 - val_loss: 1.7743 - val_accuracy: 0.5765\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.59099\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.9467 - accuracy: 0.7463 - val_loss: 1.7607 - val_accuracy: 0.5999\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.59099 to 0.59989, saving model to best_model_rnn.keras\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.8781 - accuracy: 0.7670 - val_loss: 1.8143 - val_accuracy: 0.6049\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.59989 to 0.60490, saving model to best_model_rnn.keras\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.8583 - accuracy: 0.7770 - val_loss: 1.8389 - val_accuracy: 0.5949\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.60490\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7938 - accuracy: 0.7901 - val_loss: 1.8083 - val_accuracy: 0.6283\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.60490 to 0.62827, saving model to best_model_rnn.keras\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.7486 - accuracy: 0.8089 - val_loss: 1.8310 - val_accuracy: 0.6238\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.62827\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7059 - accuracy: 0.8206 - val_loss: 1.8927 - val_accuracy: 0.6244\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.62827\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.6595 - accuracy: 0.8284 - val_loss: 1.9016 - val_accuracy: 0.6344\n",
      "\n",
      "Epoch 00033: val_accuracy improved from 0.62827 to 0.63439, saving model to best_model_rnn.keras\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.6085 - accuracy: 0.8461 - val_loss: 1.9257 - val_accuracy: 0.6311\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.63439\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.5617 - accuracy: 0.8605 - val_loss: 1.9755 - val_accuracy: 0.6199\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.63439\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.5336 - accuracy: 0.8624 - val_loss: 2.0086 - val_accuracy: 0.6266\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.63439\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.4942 - accuracy: 0.8690 - val_loss: 2.0545 - val_accuracy: 0.6138\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.63439\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.5096 - accuracy: 0.8697 - val_loss: 2.0110 - val_accuracy: 0.6272\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.63439\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.4851 - accuracy: 0.8724 - val_loss: 2.0712 - val_accuracy: 0.6177\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.63439\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.4476 - accuracy: 0.8884 - val_loss: 2.0943 - val_accuracy: 0.6377\n",
      "\n",
      "Epoch 00040: val_accuracy improved from 0.63439 to 0.63773, saving model to best_model_rnn.keras\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.4119 - accuracy: 0.8971 - val_loss: 2.0777 - val_accuracy: 0.6372\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.63773\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.3675 - accuracy: 0.9087 - val_loss: 2.0870 - val_accuracy: 0.6366\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.63773\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.3593 - accuracy: 0.9143 - val_loss: 2.1708 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.63773\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.3465 - accuracy: 0.9126 - val_loss: 2.2542 - val_accuracy: 0.6238\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.63773\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.3781 - accuracy: 0.8988 - val_loss: 2.2811 - val_accuracy: 0.6322\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.63773\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.3420 - accuracy: 0.9136 - val_loss: 2.1778 - val_accuracy: 0.6450\n",
      "\n",
      "Epoch 00046: val_accuracy improved from 0.63773 to 0.64496, saving model to best_model_rnn.keras\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2924 - accuracy: 0.9269 - val_loss: 2.3357 - val_accuracy: 0.6194\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.64496\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2881 - accuracy: 0.9331 - val_loss: 2.3039 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.64496\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2611 - accuracy: 0.9407 - val_loss: 2.3560 - val_accuracy: 0.6383\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.64496\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2372 - accuracy: 0.9464 - val_loss: 2.3854 - val_accuracy: 0.6361\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.64496\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2177 - accuracy: 0.9528 - val_loss: 2.3612 - val_accuracy: 0.6455\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.64496 to 0.64552, saving model to best_model_rnn.keras\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2200 - accuracy: 0.9488 - val_loss: 2.4415 - val_accuracy: 0.6305\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.64552\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.2086 - accuracy: 0.9518 - val_loss: 2.4141 - val_accuracy: 0.6394\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.64552\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2230 - accuracy: 0.9457 - val_loss: 2.5621 - val_accuracy: 0.6210\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.64552\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2135 - accuracy: 0.9450 - val_loss: 2.5114 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.64552\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2458 - accuracy: 0.9351 - val_loss: 2.5095 - val_accuracy: 0.6349\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.64552\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.3305 - accuracy: 0.9127 - val_loss: 2.4784 - val_accuracy: 0.6121\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.64552\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2758 - accuracy: 0.9264 - val_loss: 2.3466 - val_accuracy: 0.6444\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.64552\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2165 - accuracy: 0.9464 - val_loss: 2.3806 - val_accuracy: 0.6444\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.64552\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.1911 - accuracy: 0.9523 - val_loss: 2.4405 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.64552\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1711 - accuracy: 0.9596 - val_loss: 2.4369 - val_accuracy: 0.6433\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.64552\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1576 - accuracy: 0.9621 - val_loss: 2.4951 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.64552\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1575 - accuracy: 0.9612 - val_loss: 2.5584 - val_accuracy: 0.6283\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.64552\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1454 - accuracy: 0.9638 - val_loss: 2.5191 - val_accuracy: 0.6444\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.64552\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1324 - accuracy: 0.9635 - val_loss: 2.5643 - val_accuracy: 0.6450\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.64552\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.1297 - accuracy: 0.9649 - val_loss: 2.5661 - val_accuracy: 0.6422\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.64552\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.1309 - accuracy: 0.9637 - val_loss: 2.6341 - val_accuracy: 0.6388\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.64552\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1228 - accuracy: 0.9645 - val_loss: 2.6179 - val_accuracy: 0.6439\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.64552\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1179 - accuracy: 0.9655 - val_loss: 2.7373 - val_accuracy: 0.6294\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.64552\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1230 - accuracy: 0.9655 - val_loss: 2.6586 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.64552\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1175 - accuracy: 0.9649 - val_loss: 2.6659 - val_accuracy: 0.6361\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.64552\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1112 - accuracy: 0.9652 - val_loss: 2.7014 - val_accuracy: 0.6427\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.64552\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1133 - accuracy: 0.9652 - val_loss: 2.7183 - val_accuracy: 0.6416\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.64552\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.1119 - accuracy: 0.9644 - val_loss: 2.7652 - val_accuracy: 0.6411\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.64552\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1165 - accuracy: 0.9672 - val_loss: 2.7216 - val_accuracy: 0.6444\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.64552\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1073 - accuracy: 0.9660 - val_loss: 2.7707 - val_accuracy: 0.6422\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.64552\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0999 - accuracy: 0.9663 - val_loss: 2.7522 - val_accuracy: 0.6489\n",
      "\n",
      "Epoch 00077: val_accuracy improved from 0.64552 to 0.64886, saving model to best_model_rnn.keras\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1128 - accuracy: 0.9612 - val_loss: 2.7605 - val_accuracy: 0.6422\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.64886\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1160 - accuracy: 0.9630 - val_loss: 2.7915 - val_accuracy: 0.6394\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.64886\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1078 - accuracy: 0.9659 - val_loss: 2.8756 - val_accuracy: 0.6288\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.64886\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1260 - accuracy: 0.9609 - val_loss: 2.8296 - val_accuracy: 0.6388\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.64886\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1106 - accuracy: 0.9649 - val_loss: 2.8532 - val_accuracy: 0.6383\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.64886\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0992 - accuracy: 0.9666 - val_loss: 2.8138 - val_accuracy: 0.6439\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.64886\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0994 - accuracy: 0.9655 - val_loss: 2.7907 - val_accuracy: 0.6450\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.64886\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1103 - accuracy: 0.9646 - val_loss: 2.8212 - val_accuracy: 0.6477\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.64886\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1028 - accuracy: 0.9667 - val_loss: 2.8700 - val_accuracy: 0.6461\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.64886\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0943 - accuracy: 0.9662 - val_loss: 2.8294 - val_accuracy: 0.6528\n",
      "\n",
      "Epoch 00087: val_accuracy improved from 0.64886 to 0.65275, saving model to best_model_rnn.keras\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.1023 - accuracy: 0.9642 - val_loss: 2.8087 - val_accuracy: 0.6489\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.65275\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.1096 - accuracy: 0.9631 - val_loss: 2.7778 - val_accuracy: 0.6433\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.65275\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.0998 - accuracy: 0.9660 - val_loss: 2.7479 - val_accuracy: 0.6589\n",
      "\n",
      "Epoch 00090: val_accuracy improved from 0.65275 to 0.65888, saving model to best_model_rnn.keras\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0877 - accuracy: 0.9672 - val_loss: 2.8097 - val_accuracy: 0.6433\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.65888\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0839 - accuracy: 0.9673 - val_loss: 2.8226 - val_accuracy: 0.6522\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.65888\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0788 - accuracy: 0.9683 - val_loss: 2.8467 - val_accuracy: 0.6455\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.65888\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0893 - accuracy: 0.9680 - val_loss: 2.8033 - val_accuracy: 0.6572\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.65888\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0820 - accuracy: 0.9690 - val_loss: 2.9140 - val_accuracy: 0.6477\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.65888\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0761 - accuracy: 0.9697 - val_loss: 2.9131 - val_accuracy: 0.6439\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.65888\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0909 - accuracy: 0.9644 - val_loss: 2.9428 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.65888\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0818 - accuracy: 0.9667 - val_loss: 2.9350 - val_accuracy: 0.6444\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.65888\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.0723 - accuracy: 0.9674 - val_loss: 2.8893 - val_accuracy: 0.6483\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.65888\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.0740 - accuracy: 0.9683 - val_loss: 2.9605 - val_accuracy: 0.6388\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.65888\n",
      "training time for rnn: 143.56 초\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model_rnn.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "history_rnn = model_rnn.fit(x_train_partial, \n",
    "                            y_train_partial, \n",
    "                            epochs = 100, \n",
    "                            batch_size = 512,\n",
    "                            validation_data = (x_val, y_val),\n",
    "                            callbacks = [checkpoint])\n",
    "end_time = time.time()\n",
    "learning_time_rnn = end_time - start_time\n",
    "print(f\"training time for rnn:{learning_time_rnn: .2f} 초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80d4d838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 - 1s - loss: 2.9579 - accuracy: 0.6331\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_rnn = load_model(\"best_model_rnn.keras\")\n",
    "results_rnn = model_rnn.evaluate(x_test, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64af084a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.42      0.37        12\n",
      "           1       0.49      0.48      0.48       105\n",
      "           2       0.12      0.10      0.11        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.74      0.80      0.77       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.62      0.36      0.45        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.26      0.47      0.34        38\n",
      "           9       0.18      0.16      0.17        25\n",
      "          10       0.28      0.23      0.25        30\n",
      "          11       0.42      0.42      0.42        83\n",
      "          12       0.38      0.23      0.29        13\n",
      "          13       0.14      0.19      0.16        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.42      0.40      0.41        99\n",
      "          17       0.10      0.08      0.09        12\n",
      "          18       0.50      0.40      0.44        20\n",
      "          19       0.50      0.44      0.47       133\n",
      "          20       0.28      0.29      0.28        70\n",
      "          21       0.39      0.33      0.36        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.29      0.21      0.24        19\n",
      "          25       0.33      0.35      0.34        31\n",
      "          26       0.50      0.25      0.33         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.15      0.17      0.16        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.50      0.20      0.29        10\n",
      "          33       0.75      0.60      0.67         5\n",
      "          34       0.20      0.29      0.24         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.09      0.09      0.09        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.20      0.10      0.13        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.43      0.50      0.46         6\n",
      "          44       1.00      0.60      0.75         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.63      2246\n",
      "   macro avg       0.27      0.25      0.25      2246\n",
      "weighted avg       0.63      0.63      0.63      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model_rnn.predict(x_test)\n",
    "predicted_classes = np.argmax(predicted, axis = 1)\n",
    "print(classification_report(y_test, predicted_classes, zero_division = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1989bd1",
   "metadata": {},
   "source": [
    "# 회고(종합)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f39d2f8",
   "metadata": {},
   "source": [
    "1) 학습 시간: voting > LogisticRegression > CalibratedCV + LinearSVC > LSTM > RandomForest\n",
    "2) f1-score: LogisticRegression > Voting > CalibratedCV + LinearSVC > RandomForest > LSTM\n",
    "---\n",
    "- LogisticRegression과 Voting은 일관되게 높은 성능을 보여줌\n",
    "- LogisticRegression, CalibratedCV + LinearSVC, Voting은 vocab_size가 5000, 15000, None(+30000)으로 증가할수록 f1-score의 점수가 점차 높아짐\n",
    "- RandomForest, LSTM 두가지 모델 모두 vocab_size가 None일 때 가장 성능이 낮게 나왔으며, 두 모델 다 vocab_size = 5000과 비교 시 0.05점 이상 낮음\n",
    "---\n",
    "- LinearSVC만 단독으로 사용하는 경우 predict_proba 값이 없어 soft-voting 불가능함 (확률값 대신 결정함수 decision_function을 반환하기 때문)\n",
    "- SVC로 대체하거나 CalibratedClassifierCV를 함께 사용하여 해결 가능\n",
    "- CalibratedClassifierCV: 결정함수(decision_function)기반으로 확률값 보정\n",
    "    - 교차 검증으로 인해 추가 계산 비용이 발생하나, 일반적으로 SVC보다 학습속도가 빠름\n",
    "    - 데이터셋이 크고 선형 관계가 강할 때 사용이 적합함 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
