{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29d03bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "3.4.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bced83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cfdfe7",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cede8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "    cleaned_corpus = set(zip(kor, eng))\n",
    "    cleaned_corpus = [kor+\"\\t\"+eng for kor, eng in cleaned_corpus]\n",
    "    \n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f689a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):   \n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9가-힣?.!,]+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c135a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ko_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ko_corpus\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ko_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78967 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5192552\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1191\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78967 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 160838 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78967\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 210085\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 210085 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=84576 obj=12.9652 num_tokens=415677 num_tokens/piece=4.91483\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=71843 obj=11.8027 num_tokens=417303 num_tokens/piece=5.80854\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=53878 obj=11.8111 num_tokens=434754 num_tokens/piece=8.06923\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=53861 obj=11.7775 num_tokens=435127 num_tokens/piece=8.0787\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=40395 obj=11.9212 num_tokens=459512 num_tokens/piece=11.3755\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=40394 obj=11.8844 num_tokens=459560 num_tokens/piece=11.3769\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=30295 obj=12.0781 num_tokens=486002 num_tokens/piece=16.0423\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=30295 obj=12.0364 num_tokens=486005 num_tokens/piece=16.0424\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22721 obj=12.2795 num_tokens=513427 num_tokens/piece=22.597\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22721 obj=12.2328 num_tokens=513518 num_tokens/piece=22.601\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22000 obj=12.2621 num_tokens=516268 num_tokens/piece=23.4667\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22000 obj=12.2563 num_tokens=516335 num_tokens/piece=23.4698\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ko_corpus.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ko_corpus.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: en_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: en_corpus\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: en_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78957 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=10789012\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9506% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=38\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999506\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78957 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "uni"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 83783 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78957\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 46275\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 46275 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35194 obj=9.97208 num_tokens=87100 num_tokens/piece=2.47485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26446 obj=8.1277 num_tokens=87543 num_tokens/piece=3.31025\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21986 obj=8.04698 num_tokens=88961 num_tokens/piece=4.04626\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21887 obj=8.02745 num_tokens=89134 num_tokens/piece=4.07246\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en_corpus.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en_corpus.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3):\n",
    "    # [[YOUR CODE]]\n",
    "    temp_file = f\"{lang}_corpus.txt\"\n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input = temp_file,\n",
    "        model_prefix = f\"{lang}_corpus\",\n",
    "        vocab_size = vocab_size,\n",
    "        pad_id = pad_id,\n",
    "        bos_id = bos_id,\n",
    "        eos_id = eos_id,\n",
    "        unk_id = unk_id,\n",
    "        model_type = \"unigram\"\n",
    "    )\n",
    "    \n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.load(f\"{lang}_corpus.model\")\n",
    "    \n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair.split(\"\\t\")\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5fda40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 78968/78968 [00:04<00:00, 16703.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71288 71288\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm    # Process 과정을 보기 위해\n",
    "import tensorflow as tf\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm(range(len(kor_corpus)), desc = \"Processing data\"):\n",
    "    # [[YOUR CODE]]\n",
    "    src_tokenized = ko_tokenizer.encode_as_ids(kor_corpus[idx])\n",
    "    tgt_tokenized = en_tokenizer.encode_as_ids(eng_corpus[idx])\n",
    "    \n",
    "    if len(src_tokenized) <= 50 and len(tgt_tokenized) <= 50:\n",
    "        src_corpus.append(src_tokenized)\n",
    "        tgt_corpus.append(tgt_tokenized)\n",
    "    \n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "print(len(src_corpus), len(tgt_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4025a8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71288, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dffa4",
   "metadata": {},
   "source": [
    "# 2. 모델 설계 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f92d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2dbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        \n",
    "        #ScaledQK\n",
    "        qk = tf.matmul(Q, K, transpose_b = True)\n",
    "        scaled_qk = qk / tf.math.sqrt(d_k)\n",
    "        \n",
    "        #print(\"scaled_qk Shape:\", scaled_qk.shape)\n",
    "        #print(\"mask shape: \", mask.shape)\n",
    "        \n",
    "        if mask is not None: scaled_qk += (mask * -1e9)\n",
    "        \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis = -1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "        \n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # Embedding을 Head 수로 분할\n",
    "        # x: [batch x length x emb]\n",
    "        # return: [batch x heads x length x self.depth]\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm = [0, 2, 1, 3])\n",
    "        #print(split_x.shape, \"shape of X after split_heads\")\n",
    "        \n",
    "        return split_x\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # split된 embedding을 하나로 결합\n",
    "        # x: [batch x heads x length x depth]\n",
    "        # return: [batch x length x emb]\n",
    "        batch_size = x.shape[0]\n",
    "        #print(x.shape, \"Shape of X before combine_heads\")\n",
    "        combined_x = tf.transpose(x, perm = [0, 2, 1, 3]) # batch x length x heads x depth\n",
    "        #print(combined_x.shape, \"Shape of X after transpose before reshape\")\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "        #print(combined_x.shape, \"Shape of X after reshape\")\n",
    "        \n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        '''\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "        '''\n",
    "        # Step 1\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        #print(\"Step 1 in Mutli-head attention completed\")\n",
    "        \n",
    "        # Step 2\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        #print(\"Step 2 in Mutli-head attention completed\")\n",
    "\n",
    "        \n",
    "        # Step 3\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "        #print(\"Step 3 in Mutli-head attention completed\")\n",
    "        \n",
    "        # Step 4\n",
    "        out = self.combine_heads(out)\n",
    "        #print(\"Step 4 in Mutli-head attention completed\")\n",
    "        \n",
    "        # Step 5\n",
    "        out = self.linear(out)\n",
    "        #print(\"Step 5 in Mutli-head attention completed\")\n",
    "        \n",
    "        return out, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d213cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positionwise FeedForward\n",
    "\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation = \"relu\")\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b9bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderLayer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, mask):\n",
    "        # Multi-head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Position-wise FeedForward Network\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace867b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecoderLayer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        \n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)        \n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # Maksed Multi-head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Encoder-Decoder Multi-head Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Position-wise FeedForward Netword\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066fb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edf08a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(self.n_layers)]\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "        \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f71edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout = 0.2,\n",
    "                    shared = True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        # decoder embedding 층과 출력층의 weight sharing\n",
    "        self.shared = shared\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "            \n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1] # x: batch x seq_length\n",
    "        out = emb(x) # batch_size x seq_length x d_model\n",
    "        \n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "        \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        # self.pos_encoding[np.newaxis, ...][:, :seq_len, :].shape = [1, seq_len, d_model]\n",
    "        \n",
    "        out = self.dropout(out) # batch_size x seq_len x d_model\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, padding_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        \n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, padding_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7fa10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask 함수 정의하기\n",
    "# 주석 처리 코드 -> LMS 원본 코드\n",
    "# train_step은 돌아가는데 evaluate 할 때 scaled_qk와 shape 에러 발생함..\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# def generate_causality_mask(size):\n",
    "#     mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "#     return mask\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "# def generate_masks(src, tgt):\n",
    "#     enc_mask = generate_padding_mask(src)\n",
    "#     dec_mask = generate_causality_mask(tgt.shape[1])\n",
    "#     dec_enc_mask = generate_padding_mask(tgt)\n",
    "    \n",
    "#     return enc_mask, dec_mask, dec_enc_mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "    \n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "    \n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "    \n",
    "    return enc_mask, dec_mask, dec_enc_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61217463",
   "metadata": {},
   "source": [
    "# 3. 훈련하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4b74e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터값 조정\n",
    "n_layers = 2\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "max_len = 50\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a6dc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers = n_layers,\n",
    "    d_model = d_model,\n",
    "    n_heads = n_heads,\n",
    "    d_ff = d_ff,\n",
    "    src_vocab_size = SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size = TGT_VOCAB_SIZE,\n",
    "    pos_len = max_len,\n",
    "    dropout = 0.2,\n",
    "    shared = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84104179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler & Optimizer\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps = 4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    \n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4375421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True, reduction = 'none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7755371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:] # bos token 제외하고 ~\n",
    "    \n",
    "    enc_mask, dec_mask, dec_enc_mask = generate_masks(src, tgt)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_mask, dec_enc_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "        \n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6bfd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, dec_causality_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              dec_causality_mask,\n",
    "              dec_padding_mask)\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8689246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb7e2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb4959f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= [\n",
    "    '오바마는 대통령이다.',\n",
    "    '시민들은 도시 속에 산다.',\n",
    "    '커피는 필요 없다.',\n",
    "    '일곱 명의 사망자가 발생했다.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15d22aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  1: 100%|██████████| 1114/1114 [03:37<00:00,  5.12it/s, LOSS 5.9608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is the president .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. the blasts were not to the deaths .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. the new york is not to be a lot of the new york .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the blasts were killed .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  1\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  2: 100%|██████████| 1114/1114 [03:31<00:00,  5.26it/s, LOSS 4.4040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a president .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. the city city is the city of urban city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. if you don t have any excuse .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the deadly deadly deadly .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  2\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  3: 100%|██████████| 1114/1114 [03:31<00:00,  5.27it/s, LOSS 3.7878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a lot of president .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. the people are the living .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. there is no sign of no .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. a tornado hit the dead , killing at least one person died .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  3\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  4: 100%|██████████| 1114/1114 [03:31<00:00,  5.28it/s, LOSS 3.3773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is the president .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. the citizens are everywhere .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. coffee needs , which are not a coffee .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the death toll was killed by a local toll .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  4\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  5: 100%|██████████| 1114/1114 [03:30<00:00,  5.29it/s, LOSS 2.8294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a president .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are huges .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. no need for the need .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seventh died .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  5\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  6: 100%|██████████| 1114/1114 [03:30<00:00,  5.28it/s, LOSS 2.2110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. he is a president .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are looking for their urban .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. no need for .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. seven people were killed in the event .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  6\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  7: 100%|██████████| 1114/1114 [03:30<00:00,  5.28it/s, LOSS 1.6656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a state . . . . . . .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are proud of their city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. the need need for . . . needs .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seven crew killed seven people .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  7\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  8: 100%|██████████| 1114/1114 [03:31<00:00,  5.28it/s, LOSS 1.2190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a real person .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. they re just caught in the city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. no need for faa need .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. seven people are watching television .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  8\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH  9: 100%|██████████| 1114/1114 [03:31<00:00,  5.28it/s, LOSS 0.8825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a president .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are drawn into the city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. the protect youed explorer .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. seven people died in the hospital .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  9\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 10: 100%|██████████| 1114/1114 [03:31<00:00,  5.28it/s, LOSS 0.6509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a matter of u . s .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are busy considers destroying urban violent .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. the need needed to make a fight .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seven death force is making the remainder .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  10\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 11: 100%|██████████| 1114/1114 [03:31<00:00,  5.28it/s, LOSS 0.5036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a test .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are located about our city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. we need need to need need , or a doctor neededal needs .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seven was among the dead .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  11\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 12: 100%|██████████| 1114/1114 [03:31<00:00,  5.28it/s, LOSS 0.4066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a real man .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are located .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. the need for religious democracy .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seven death toll is in the hospital .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  12\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 13: 100%|██████████| 1114/1114 [03:30<00:00,  5.28it/s, LOSS 0.3394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a test .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. they detained on the city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. the need needed to protect you .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. seven people died in the violence .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  13\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 14: 100%|██████████| 1114/1114 [03:30<00:00,  5.28it/s, LOSS 0.2901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a response for president bush .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. they are showing aware of anti cities .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. the need for you , did not need to speak to your average .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seventh death toll is reported .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  14\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 15: 100%|██████████| 1114/1114 [03:31<00:00,  5.28it/s, LOSS 0.2508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a real .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are showing the life race .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. so we need to go with .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. seven people died in the blast .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  15\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 16: 100%|██████████| 1114/1114 [03:30<00:00,  5.28it/s, LOSS 0.2207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. the president elect is a president .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. they want to name town town .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. coffee needs .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seven died of injuries .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  16\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 17: 100%|██████████| 1114/1114 [03:30<00:00,  5.28it/s, LOSS 0.1971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a response from the united states .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are free to visitors .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. no need for . . . . . . . . .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seventh died in that number .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  17\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 18: 100%|██████████| 1114/1114 [03:31<00:00,  5.27it/s, LOSS 0.1762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. the president is real .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. they want to protect the city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. no arrests or needed .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seven people were killed .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  18\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 19: 100%|██████████| 1114/1114 [03:31<00:00,  5.27it/s, LOSS 0.1584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a real man .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. some citizens are caught in the city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. no need for coffee .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seven people are watching the deaths .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  19\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 20: 100%|██████████| 1114/1114 [03:31<00:00,  5.27it/s, LOSS 0.1444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "> input : 1. 오바마는 대통령이다.\n",
      "> output: 1. obama is a real guy .\n",
      "> input : 2. 시민들은 도시 속에 산다.\n",
      "> output: 2. citizens are showing 20 people located in the city .\n",
      "> input : 3. 커피는 필요 없다.\n",
      "> output: 3. the need forness is needed .\n",
      "> input : 4. 일곱 명의 사망자가 발생했다.\n",
      "> output: 4. the seven people died in the attack .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers:  2\n",
      "d_model:  512\n",
      "n_heads:  8\n",
      "d_ff:  2048\n",
      "dropout:  0.2\n",
      "\n",
      "Training Parameters\n",
      "Warmup Steps:  4000\n",
      "Batch Size:  64\n",
      "Epoch At:  20\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    \n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, _, _, _ = train_step(enc_train[idx: idx + BATCH_SIZE],\n",
    "                                dec_train[idx: idx + BATCH_SIZE],\n",
    "                                transformer,\n",
    "                                optimizer,)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str(\"EPOCH %2d\" % (epoch + 1))\n",
    "        t.set_postfix_str('LOSS %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    print(\"Translations\")   \n",
    "    for idx, text in enumerate(sample_text):\n",
    "        \n",
    "        pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        evaluate(text, transformer, ko_tokenizer, en_tokenizer)\n",
    "        \n",
    "        print(f\"> input : {idx+1}. {text}\")\n",
    "        print(f\"> output: {idx+1}. {result}\")\n",
    "        \n",
    "    print(\"\\nHyperparameters\")\n",
    "    print(\"n_layers: \", n_layers)\n",
    "    print(\"d_model: \", d_model)\n",
    "    print(\"n_heads: \", n_heads)\n",
    "    print(\"d_ff: \", d_ff)\n",
    "    print(\"dropout: \", dropout)\n",
    "        \n",
    "    print(\"\\nTraining Parameters\")\n",
    "    print(\"Warmup Steps: \", learning_rate.warmup_steps)\n",
    "    print(\"Batch Size: \", BATCH_SIZE)\n",
    "    print(\"Epoch At: \", epoch+1)\n",
    "    print(\"-------------------------------------------\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f26e19",
   "metadata": {},
   "source": [
    "# 회고 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbf23c",
   "metadata": {},
   "source": [
    "- 학습 속도가 1 epoch당 3분 30초 걸렸으며, 이전 S2S 모델이 약 10분정도 걸렸던 것에 비하면 속도는 훨씬 빠른 것 같음\n",
    "- Loss값도 5.9608 -> 0.1444 로 안정적으로 떨어진 모습이 나타남\n",
    "- 하지만 모델의 성능 자체는 epoch이 늘어남에 따라 비례해서 올라가는 것은 아닌 것 같고, 변동성있는 모습이 보임\n",
    "- Generate Mask 쪽이 원본 코드와 변경되었는데, 추가적인 이해가 필요할 것 같다! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
