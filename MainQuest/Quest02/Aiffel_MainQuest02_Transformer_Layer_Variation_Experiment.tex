% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{emnlp2021}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Impact of Transformer Depth on Small Datasets for Chatbot Models}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Saehee Park\\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{sarah4282@gmail.com} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
This study investigates the impact of transformer layer variations (2, 4, 8, and 16 layers) on the performance and efficiency of chatbot models trained on a small curated dataset. From the original DailyDialog dataset containing 76,052 samples, 10,000 instances were selected after preprocessing, including duplicate removal and filtering by token count (3â€“60). Evaluation metrics include BLEU, ROUGE, METEOR, and qualitative assessments. Results indicate that the 8-layer transformer achieves the best balance between response quality and computational efficiency. These findings highlight optimal configurations for transformer models in low-resource conversational AI.
\end{abstract}

\section{Introduction}

Modern NLP models, particularly transformers, have demonstrated exceptional performance across tasks with large-scale datasets. However, challenges arise when applying such architectures to small datasets. This study evaluates the impact of varying transformer depth (2, 4, 8, and 16 layers) on model performance and computational efficiency in a small-data setting.


\section{Background and Related Works}
TBU

\section{Method}
\subsection{Dataset Preprocessing}
The experiments were conducted using the DailyDialog dataset, a widely-used datasets for dialog generation tasks. The original dataset consists of 76,052 multi-turn conversational samples. To dapt the data for this study, the following preprocessing steps were applied:
    \begin{enumerate}
        \item \textbf{Duplicate Removal}: All duplicate samples were removed to ensure diversity in the training data and minimize inference of perplexity in model training.
        \item \textbf{Token-length Filtering}: Dialog samples were filtered to retain only those with token counts between 3 and 60, ensuring meaningful yet manageable input lengths. 95\% of total training datasets are included in this range.
        \item \textbf{Sample Selection}: A subset of 10,000 instances was randomly selected from the preprocessed dataset to simulate a small-scale training scenario.
        \item \textbf{Tokenization}: Tokenization was performed using the NLTK punkt tokenizer and a vocabulary size of 15,000 tokens was constructed.
    \end{enumerate}

\subsection{Transformer Model Configuration}
The configuration of the model follows the standard transformer design with a slight modification in embedding dimension size and variation in encoder and decoder layer numbers, ranging from 2, 4, 8 to 16. 
\begin{itemize}
    \item \textbf{Embedding Dimension ($d_{model}$):} 256. The weights of the embedding layers in the encoder and decoder are shared, since the source and target languages are the same (English).
    \item \textbf{Number of Attention Heads ($n_{heads}$):} 8.
    \item \textbf{Feedforward Layer Dimension ($d_{ff}$):} 1024.
    \item \textbf{Dropout Rate:} 0.1.
    \item \textbf{Positional Encoding:} Maximum sequence length of 128 tokens.
\end{itemize}

\subsection{Training Setup}
The models were trained using the Adam optimizer with the following hyperparameters:
    \begin{itemize}
        \item \textbf{Epochs per model}: 10
        \item \textbf{Learning Rate Scheduler}: Warm-up steps of 4,000, followed by inverse square root decay.
        \item \textbf{Optimizer Parameters}:
            \begin{center}
                $\beta_1$ = 0.9, $\beta_2$ = 0.98, $\epsilon$ = $1e-9$
            \end{center}
        \item \textbf{Batch Size}: 64
        \item \textbf{Loss Function}: Sparse Categorical Crossentropy, with a padding mask applied to ignore <pad> tokens in the input and target sequences.
    \end{itemize}
    
\subsection{Evaluation Metrics}
To assess model performance and training efficiency, following evaluations were performed:
    \begin{enumerate}
        \item \textbf{Quantitative Metrics for Text Generation}\\   
        The 3 following evaluation metrics have scores between 0 to 1, and score above 0.2 is suggested for generative models.
          \begin{itemize}
            \item \textbf{BLEU}: Measures n-gram overlap between generated responses and ground truth. 
            \item \textbf{ROUGE}: Evaluates overlap of longer sequences, and ROUGE-L was used for the experiments.
            \item \textbf{METEOR}: Consider semantic similarity, penalizing over-reliance on exact matches
        \end{itemize}
        \item \textbf{Computational Efficiency Metrics}:
            \begin{itemize}
                \item Training Time Per Epoch: Time taken to complete one training epoch
                \item Convergence Speed: Number of epochs required to achieve optimal performance
            \end{itemize}
    \end{enumerate}
  

\section{Result}



\section{Conclusion}
TBU

\section{Acknowledgements}
TBU

\section{References}
\label{sec:appendix}

This is an appendix.

\end{document}
